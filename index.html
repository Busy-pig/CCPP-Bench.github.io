<!DOCTYPE html>
<html>
<head>
  <title>CCPP-Bench</title>
    <style>
        .hidden {
            display: none;
        }
        h2.title.is-3 {
            background-color: #e8f5e9; /* 浅绿色背景 */
            padding: 15px;  /* 内边距 */
            border-radius: 5px; /* 圆角 */
            margin: 15px 0; /* 上下外边距 */
            transition: background-color 0.3s ease; /* 平滑过渡效果 */
        }
    </style>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://kit.fontawesome.com/f8ddf9854a.js" crossorigin="anonymous"></script>
  <meta charset="utf-8">
  <meta name="description" content="Can MLLMs Understand the Deep Implication Behind Chinese Images?">
  <meta name="keywords" content="CCPP-Bench, LMM, LMM Evaluation, MLLM, MLLM Evaluation, Multimodal large language model, Vision Language Model, Large Language Model, Large Multimodal Model, artificial intelligence, AI, AGI, artificial general intelligence">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Can MLLMs Understand the Deep Implication Behind Chinese Images?</title>

  <link rel="icon" href="./images/icon.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer=""></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer="" src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/question_card.js"></script>
<!--  <script src="./data/results/data_setting.js" defer></script>-->
<!--  <script src="./data/results/model_scores.js" defer></script>-->
<!--  <script src="./visualizer/data/data_public.js" defer></script>-->
</head>
<body class="CCPP-Bench-container">

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <span class="CCPP-Bench" style="vertical-align: middle">CCPP-Bench</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Evaluating and Benchmarking Classical Chinese Poetry-to-Painting for
Multimodal Large Language Models
          </h2>
          <p style="color: red; font-size: 1.5em; font-weight: bold;">ACL 2026</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal large language models (MLLMs) have demonstrated impressive capabilities in 
generating high-quality images from modern text. However, their performance in Classical Chinese Poetry-to-Painting (CCPP) generation, a vital and enduring part of Chinese literature, remains underexplored. To fill this gap, we propose a novel evaluation benchmark CCPP-Bench, which aims to evaluate the model’s poetry-to-painting generation conditioned on classical Chinese poems. Specifically, we first collect 1,079 correct <poetry, human-painting> pairs, along with their metadata attributes (e.g., dynasty, theme, and explanation). We then utilize 4 representative MLLMs to produce paintings in two input modes (i) poetry-only and (ii) enhancing poetry with explanation. We employ a panel of 8 human experts to assess a total of 8,632 pairs of poetry and model-painting in terms of 6 key dimensions specifically focused on visual quality, faithfulness to poetry, and cultural precision. We comprehensively investigate 6 research questions (RQs) that reveal significant progress and persistent challenges in this task. The study not only promotes the dissemination of ancient poetry culture, but also offers a multimodal creative paradigm “poetry- to-painting” to enhance LLM evaluations. We release CCPP-Bench on Anonymous Github.
          </p>
        </div>
      </div>
    </div>
    
  </div>
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            The CCPP project is designed to facilitate research on "Classical Chinese Poetry-to-Painting" generation. It provides a high-quality benchmark, human-created painting samples, MLLM-generated painting outputs, and tools for data processing/evaluation. The project aims to enable reproducible research on evaluating MLLMs’ understanding of classical Chinese culture (poetry, idioms, classical prose) and their cross-modal generation capabilities. We provide a partial dataset (300 samples) and human-painting. The full dataset will be released upon paper acceptance.CCPP-Bench

        </p></div>
    </div>
    </div>

    <div class="columns is-centered m-6">
  <div class="column is-full has-text-centered content">
    <h2 class="title is-3">Statistics</h2>

    <table class="table is-bordered is-striped is-hoverable is-centered">
      <thead>
        <tr>
          <th>Item</th>
          <th>Number</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Total poems</strong></td>
          <td>1,079</td>
        </tr>
        <tr>
          <td>&nbsp;&nbsp;– Total idioms</td>
          <td>382</td>
        </tr>
        <tr>
          <td>&nbsp;&nbsp;– Total Shi &amp; Ci</td>
          <td>466</td>
        </tr>
        <tr>
          <td>&nbsp;&nbsp;– Total Wen Yan Wen</td>
          <td>231</td>
        </tr>

        <tr>
          <td><strong>Total Chinese characters</strong></td>
          <td>29,565</td>
        </tr>
        <tr>
          <td>Chinese characters per poem</td>
          <td>Min: 4; Max: 84; Avg: 27.4</td>
        </tr>

        <tr>
          <td><strong>Total paintings</strong></td>
          <td>9,711</td>
        </tr>
        <tr>
          <td>&nbsp;&nbsp;– Total human-painting</td>
          <td>1,079</td>
        </tr>
        <tr>
          <td>&nbsp;&nbsp;– Total model-painting</td>
          <td>8,632</td>
        </tr>

        <tr>
          <td><strong>Size of painting (MB)</strong></td>
          <td>Min: 0.41; Max: 10.02; Avg: 1.5</td>
        </tr>
      </tbody>
    </table>
      <figure class="image is-inline-block">
      <img src="images/image7.png" alt="The distribution of keywords" style="max-width: 90%;">
    </figure>
    <p class="mt-3">
      The distribution of keywords
    </p>
    </table>
      <figure class="image is-inline-block">
      <img src="images/image8.png" alt="The distribution of keywords" style="max-width: 90%;">
    </figure>
    <p class="mt-3">
      Fine-grained statistics of CCPP-Bench (one-level), i.e., the total number of samples in terms of different meta-data.
In particular, we list meta-data attributes classified into poetry-related and painting-related.
    </p>
    </table>
      <figure class="image is-inline-block">
      <img src="images/image9.png" alt="Fine-grained statistics of CCPP-Bench (two-level). Top: type (primary) and dynasty (secondary); Middle: type
(primary) and theme (secondary); Bottom: painting techniques (primary) and painting objects (secondary)." style="max-width: 90%;">
    </figure>
    <p class="mt-3">
      Fine-grained statistics of CCPP-Bench (two-level). Top: type (primary) and dynasty (secondary); Middle: type
(primary) and theme (secondary); Bottom: painting techniques (primary) and painting objects (secondary).
    </p>
  </div>
</div>

</section>



<!-- ------------------------------------------------------------------ RESULTS SECTION ------------------------------------------------------------------ -->
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
        <div class="content">
          <div class="content has-text-justified">
            <p>
              We conduct systematic experiments on both open-source and closed-source MLLMs using CCPP-Bench. For each model, we employ eight different configurations: None (zero-shot), 1-shot, 2-shot,
              3-shot, CoT, Domain, Emotion, and Rhetoric. “None” represents the use of a standard prompt
              without any additional information. “Emotion” indicates the inclusion of information related to the
              emotional polarity of the image (e.g., positive, negative) in the prompt, “Domain” involves adding
              information about the image’s domain (e.g., life, art), and “Rhetoric” refers to including details
              about the rhetorical devices used in the image (e.g., metaphor, contrast) in the prompt. Additionally,
              to verify the necessity of images in problem-solving, we select a portion of LLMs to complete tasks
              without image input.
            </p>
          </div>

          <div class="model-labels-container">

            <span class="leaderboard-label" style="background-color: #def9cb;">Closed-Source</span>
            <span class="leaderboard-label" style="background-color: #e4efdc;">Open-Source</span>
            <span class="leaderboard-label" style="background-color: #f6d066;">Text-Only</span>
            <span class="leaderboard-label" style="background-color: #e0ebf3;">Human</span>

          </div>
          <table id="table1" class="js-sort-table">
            <tbody><tr>
              <td class="js-sort-number"><strong>Model</strong></td>
              <td class="js-sort-number"><strong>Overall</strong></td>
              <td class="js-sort-number"><strong>Life</strong></td>
              <td class="js-sort-number"><strong>Art</strong></td>
              <td class="js-sort-number"><strong>Society</strong></td>
              <td class="js-sort-number"><strong>Politics</strong></td>
              <td class="js-sort-number"><strong>Environment</strong></td>
              <td class="js-sort-number"><strong>Chinese Traditional Culture</strong></td>
              <td class="js-sort-number"><strong>Positive</strong></td>
              <td class="js-sort-number"><strong>Negative</strong></td>
              <td class="js-sort-number"><strong>Neutral</strong></td>
            </tr>

              <tr style="background-color: #def9cb;">
                <td style="text-align: left;">
                    <b> GLM-4V </b>
                </td>
                <td><b>60.9</b></td>
                <td><u>55.0</u></td>

                <td>59.9</td>
                <td><b>66.5</b></td>
                <td><u>66.7</u></td>
                <!-- <td style="text-decoration: underline;"> 66.7 </td> -->
                <td><b>79.3</b></td>
                <td><b>55.5</b></td>
                <td><b>58.5</b></td>
                <td><u>64.5</u></td>
                <td><b>59.4</b></td>
              </tr>
            
              <tr style="background-color: #def9cb;">
                <td style="text-align: left;">
                      <b> Gemini-1.5 Pro </b>
                </td>
                <td><u>60.1</u></td>
                <td><b>60.0</b></td>
                
                <td><b>63.3</b></td>
                <td><u>62.4</u></td>
                <td><b>70.8</b></td>
                <td>62.1</td>
                <td>51.1</td>
                <td><u>54.8</u></td>
                <td><b>65.6</b></td>
                <td><b>59.4</b></td>
              </tr>

              <tr style="background-color: #def9cb;">
                <td style="text-align: left;">
                    <b> Qwen-VL-MAX </b>
                </td>
                <td>56.9</td>
                <td>53.3</td>
                
                <td>59.2</td>
                <td>58.8</td>
                <td>62.5</td>
                <td><u>67.2</u></td>
                <td>52.6</td>
                <td>53.9</td>
                <td>58.3</td>
                <td>58.0</td>
              </tr>


              <tr style="background-color: #def9cb;">
                <td style="text-align: left;">
                    <b> Claude-3.5-Sonnet </b>
                </td>
                <td>54.1</td>
                <td>52.1</td>
                
                <td><u>61.9</u></td>
                <td>52.6</td>
                <td>62.5</td>
                <td>46.6</td>
                <td><u>53.3</u></td>
                <td>52.7</td>
                <td>56.5</td>
                <td>53.0</td>
              </tr>

              <tr style="background-color: #def9cb;">
                <td style="text-align: left;">
                        <b> GPT-4o </b>
                </td>
                <td>54.1</td>
                <td>54.1</td>

                <td>55.8</td>
                <td>52.1</td>
                <td>50.0</td>
                <td>63.8</td>
                <td>51.8</td>
                <td>51.9</td>
                <td>56.2</td>
                <td>54.1</td>
              </tr>


              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                    <b> Qwen2-VL-72B </b>
                </td>
                <td><b>64.4</b></td>
                <td><b>61.7</b></td>

                <td><b>61.2</b></td>
                <td><b>68.0</b></td>
                <td><b>79.2</b></td>
                <td><b>75.9</b></td>
                <td><b>59.9</b></td>
                <td><b>62.7</b></td>
                <td><b>63.8</b></td>
                <td><b>66.4</b></td>
              </tr>

              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                        <b> InternVL2-40B </b>
                </td>
                <td><u>57.9</u></td>
                <td><u>55.8</u></td>

                <td><u>55.1</u></td>
                <td><u>61.9</u></td>
                <td>62.5</td>
                <td><u>70.7</u></td>
                <td><u>52.6</u></td>
                <td>54.4</td>
                <td><u>58.0</u></td>
                <td><u>60.8</u></td>
              </tr>

              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                      <b> InternVL2-8B </b>
                </td>
                <td>53.1</td>
                <td>49.2</td>
                
                <td>53.1</td>
                <td>55.7</td>
                <td>62.5</td>
                <td>63.8</td>
                <td>50.4</td>
                <td>50.6</td>
                <td>53.3</td>
                <td>55.1</td> 
              </tr>

              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                      <b>InternVL2-Llama3-76B</b>
                </td>
                <td>52.9</td>
                <td>50.8</td>

                <td>53.7</td>
                <td>51.0</td>
                <td>58.3</td>
                <td>67.2</td>
                <td>51.1</td>
                <td><u>54.8</u></td>
                <td>51.8</td>
                <td>52.3</td>
              </tr>

              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                      <b>GLM-4V-9b</b>
                </td>
                <td>50.3</td>
                <td>46.7</td>

                <td>48.3</td>
                <td>53.6</td>
                <td>54.2</td>
                <td>62.1</td>
                <td>48.2</td>
                <td>51.9</td>
                <td>52.9</td>
                <td>46.3</td>
              </tr>

              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                      <b>Qwen2-VL-7B</b>
                </td>
                <td>49.6</td>
                <td>42.5</td>

                <td>51.7</td>
                <td>54.1</td>
                <td>62.5</td>
                <td>65.5</td>
                <td>44.5</td>
                <td>50.2</td>
                <td>47.5</td>
                <td>51.2</td>
              </tr>

              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                      <b>LLaVA-1.6-72B</b>
                </td>
                <td>48.0</td>
                <td>43.8</td>

                <td>48.3</td>
                <td>49.5</td>
                <td><u>70.8</u></td>
                <td>60.3</td>
                <td>43.8</td>
                <td>41.5</td>
                <td>52.5</td>
                <td>49.2</td>
              </tr>

              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                      <b>LLaVA-1.6-34B</b>
                </td>
                <td>46.0</td>
                <td>40.8</td>

                <td><u>55.1</u></td>
                <td>42.8</td>
                <td>45.8</td>
                <td>62.1</td>
                <td>43.1</td>
                <td>44.4</td>
                <td>48.2</td>
                <td>45.2</td>
              </tr>

              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                      <b>MiniCPM-v2.6</b>
                </td>
                <td>45.0</td>
                <td>37.5</td>

                <td>47.6</td>
                <td>49.5</td>
                <td>58.3</td>
                <td>55.2</td>
                <td>42.3</td>
                <td>45.6</td>
                <td>44.6</td>
                <td>44.9</td>
              </tr>

              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                      <b>CogVLM2-Llama3-Chinese-Chat</b>
                </td>
                <td>43.4</td>
                <td>37.1</td>

                <td>48.3</td>
                <td>42.3</td>
                <td>54.2</td>
                <td>63.8</td>
                <td>40.2</td>
                <td>40.3</td>
                <td>45.7</td>
                <td>43.8</td>
              </tr>

              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                      <b>MiniCPM-Llama3-2.5</b>
                </td>
                <td>40.4</td>
                <td>36.3</td>

                <td>45.6</td>
                <td>37.1</td>
                <td>50.0</td>
                <td>51.7</td>
                <td>40.2</td>
                <td>43.2</td>
                <td>37.0</td>
                <td>41.3</td>
              </tr>

              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                      <b>idefics2-8b</b>
                </td>
                <td>36.3</td>
                <td>25.0</td>

                <td>46.3</td>
                <td>38.1</td>
                <td>41.7</td>
                <td>56.9</td>
                <td>32.9</td>
                <td>32.8</td>
                <td>39.1</td>
                <td>36.4</td>
              </tr>

              <tr style="background-color: #e4efdc;">
                <td style="text-align: left;">
                      <b>Qwen-VL-Chat</b>
                </td>
                <td>34.3</td>
                <td>27.9</td>

                <td>34.7</td>
                <td>32.5</td>
                <td>45.8</td>
                <td>55.2</td>
                <td>36.5</td>
                <td>34.0</td>
                <td>35.1</td>
                <td>33.6</td>
              </tr>


              <tr style="background-color: #f6d066;">
                <td style="text-align: left;">  
                      <b> Qwen2-7B-Instruct </b>
                </td>
                <td><b>32.5</b></td>
                <td><b>33.2</td>

                <td><b>34.6</b></td>
                <td><b>30.9</b></td>
                <td><b>35.0</b></td>
                <td><b>40.7</b></td>
                <td><b>28.5</b></td>
                <td><b>33.6</b></td>
                <td><b>30.4</b></td>
                <td><b>33.6</b></td>
              </tr>  
              
              <tr style="background-color: #f6d066;">
                <td style="text-align: left;">
                      <b> DeepSeek-67B-Chat </b>
                </td>
                <td><u>27.1</u></td>
                <td><u>26.6</td>
                  
                <td><u>32.7</td>
                <td><b>30.9</b></td>
                <td>20.0</td>
                <td><u>35.2</u></td>
                <td>18.2</td>
                <td><u>25.7</u></td>
                <td>22.2</td>
                <td><u>33.2</u></td>
              </tr>  

              <tr style="background-color: #f6d066;">
                <td style="text-align: left;">
                      <b> Llama-3-8B-Instruct </b>
                </td>
                <td>21.7</td>
                <td>22.2</td>

                <td>26.9</td>
                <td>18.6</td>
                <td><u>25.0</u></td>
                <td>27.8</td>
                <td><u>20.4</u></td>
                <td>21.2</td>
                <td><u>24.4</u></td>
                <td>19.5</td>
              </tr>  


              <tr style="background-color: #e0ebf3;">
                <td style="text-align: left;">
                      <b> Human_avg </b>
                </td>
                <td>78.2</td>
                <td>81.0</td>

                <td>67.7</td>
                <td>82.7</td>
                <td>87.7</td>
                <td>84.0</td>
                <td>65.9</td>
                <td>77.9</td>
                <td>75.2</td>
                <td>81.6</td>
              </tr>

              <tr style="background-color: #e0ebf3;">
                  <td style="text-align: left;">
                        <b> Human_best </b>
                  </td>
                  <td><b>81.0</b></td>
                  <td><b>83.2</b></td>

                  <td><b>73.6</b></td>
                  <td><b>87.2</b></td>
                  <td><b>89.5</b></td>
                  <td><b>86.0</b></td>
                  <td><b>66.7</b></td>
                  <td><b>78.2</b></td>
                  <td><b>78.8</b></td>
                  <td><b>83.3</b></td>
              </tr>
          </tbody></table>

            <p> Overall results of different MLLMs, LLMs and humans on different domains and emotions. The
                best-performing model in each category is <b>in-bold</b>, and the second best is <u>underlined</u>.
            </p>
        </div>
      </div>
    </div>

<!-------------------------------------------------------------------- prompt skills -------------------------------------------------------------------->
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Different Prompt Skills</h2>
        <div class="content has-text-justified">
          <p>
          <b>Analysis of Chain-of-Thought (CoT)</b>. The results indicate that CoT does not significantly improve the accuracy of the models. 
            In some cases, particularly with smaller open-source models, the accuracy even declined when CoT was used. 
            For example, MiniCPM-v2.6 scores 45.0% without CoT, but this drops to 38.9% with CoT; similarly, LLaVA-1.6-72B scores decrease from 48.0% to 45.3%.
            Upon analyzing the models’ responses, we find that those models showing a decrease in accuracy with CoT often suffer from overinterpretation, where questions that were initially answered correctly are misinterpreted after CoT is applied. 
            Additionally, for questions that were originally answered incorrectly, CoT does not lead to significant improvements and sometimes even causes confusion, such as selecting multiple options. 
            However, for most models, the probability of failing to extract an answer option from the response decreases after using CoT, which explains why some models show improved accuracy with CoT.
          </p>
          <p>
          <b>Analysis of Different Types and Domains</b>. To evaluate the impact of different label information on model accuracy, we conduct an ablation study by providing relevant label information(Emotion, Domain, Rhetoric) in the prompts. 
            The results show that emotion labels significantly improve model accuracy, followed by domain and rhetoric labels, both of which exhibit similar effectiveness.
            This result aligns with human intuition. The answer options typically include negative, positive, and neutral choices. When the model receives emotional information, it can eliminate some irrelevant options, naturally leading to higher accuracy.
            In contrast, domain and rhetoric information generally do not effectively help the model eliminate options, resulting in more limited improvements.
            Additionally, from a model training perspective, models tend to have a more mature understanding of emotions, while specific nouns in rhetoric and domain labels are often custom-defined. 
            During pre-training, the model may not have encountered a large number of descriptions for such specific nouns, making these labels less helpful in improving accuracy.
          </p>
          <div class="content has-text-centered">
            <img src="images/prompt.png" width="75%">
          <p>Overall results of different prompts on CCPP-Bench. The label(Emotion, Domain, Rhetoric) means providing corresponding information for the images in the prompt.
            The best-performing model in each category is <b>in-bold</b>, and the second best is <u>underlined</u> .</p>
          </div>

          <p>
          <b>Analysis of Few-shot Examples</b>. The results indicate that few-shot examples do not improve the models’ accuracy. 
          Specifically, performance declines as the number of examples increases. This decline can be attributed to the models’ inferior capabilities in handling multiple images compared to single images, leading to a decrease in accuracy with a higher number of shots.
          Furthermore, as the number of shots increases, the input length also extends, and the models’ ability to process long texts is inadequate, resulting in suboptimal performance with long contexts.
        </div>
        <div class="content has-text-centered">
          <img src="images/fewshot.png" width="73%">
          <p>Few-shot results of different models on the CCPP-Bench.</p>
        </div>

      </div>
    </div>
<!-------------------------------------------------------------------- CTC Evaluation SECTION -------------------------------------------------------------------->
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Chinese Traditional Culture Evaluation</h2>
        <div class="content has-text-justified">
          <p>
          <b>Why Choose to Evaluate Chinese Traditional Culture?</b> The Chinese traditional culture category is a distinctive feature of the CCPP-Bench dataset, where MLLMs consistently score the lowest. 
            Therefore, we need a deeper evaluation of this field to analyze the extent to which MLLM understands Chinese traditional culture.
            We choose to deeply analyze MLLM’s understanding of Chinese traditional culture by evaluating Chinese traditional paintings.
          </p>
          <p>
            <b>Why Choose Chinese Traditional Paintings?</b> The imagery associated with Chinese traditional culture often embodies complex implications, encompassing customs, historical anecdotes, and legendary tales, making direct evaluation particularly challenging. Chinese traditional painting, intrinsically intertwined with Chinese traditional culture, offers a viable proxy for this assessment. 
            The unique value of Chinese traditional painting lies in its embodiment of Chinese cultural connotations, aesthetic implications, and distinctive artistic expression.
            The core philosophical concepts of Confucianism, Taoism, and Buddhism, along with their humanistic essence, have consistently permeated the entire trajectory of Chinese painting history.
            Consequently, we choose to evaluate MLLMs’ comprehension of Chinese traditional culture through an in-depth analysis of their understanding of Chinese traditional paintings.
            </p>
          <p>
          <b>Evaluation Metric</b>. Chinese traditional painting, a cornerstone of Chinese traditional culture, encompasses a rich tapestry of styles and techniques developed over millennia. 
            These paintings are typically categorized based on their subject matter (e.g., landscape paintings, flower-and-bird paintings, figure paintings, and New Year paintings) or their stylistic and skill (e.g., court paintings, meticulous brush paintings, freehand brush paintings, and color-and-ink paintings). 
            Each category embodies unique characteristics that reflect China’s artistic evolution and philosophical underpinnings.
            To comprehensively assess MLLMs’ understanding of Chinese traditional paintings, we develop a multifaceted evaluation metric. This metric is designed to probe both the surface-level information readily apparent in the artwork and the deeper culture and history that informs its creation and interpretation. 
            Our evaluation metric encompasses five key perspectives: <u>Surface-level Information</u>, <u>Aesthetic Characteristics</u>, <u>Brush and Ink Skills</u>, <u>Culture and History</u>, and <u>Deep Implications</u>.
          </p>
          <div class="content has-text-centered">
            <img src="images/evaluation_metric_and_standard.png" width="73%">
          <p>Evaluation metric and evaluation standard of Chinese traditional painting.</p>
          </div>
          <p>
          <b>LLM-Based Chinese Traditional Painting Automatic Evaluation</b>. Our experiment utilize the CTC domain data from CCPP-Bench, comprising 130 Chinese traditional paintings.
          We employ human-written descriptions and implication interpretations as ground truth. 
          We choose GPT-4o to generate descriptions for these images, which are subsequently scored using GPT-4o and our evaluation standard.
          To validate the model’s scoring efficacy, we enlist three PhD students well-versed in Chinese metaphorical imagery to independently score the 130 paintings.
          The model-human scoring consistency reached 98%, affirming the method’s validity for assessing Chinese traditional painting comprehension. 
          Analysis of these results, in conjunction with our evaluation standard, reveals insights across three dimensions: overall performance, difficulty levels, and emotions. 
          The overall score of 2.71 indicates that while MLLM is able to observe the surface-level information of paintings, it has a large gap with humans in deeply interpreting the complex cultural elements contained in Chinese traditional art. 
          In terms of difficulty evaluation, the model is consistent with human cognition, while in terms of emotion, the model has a higher negative score, indicating that the model can identify negative implications in paintings, such as using the past to satirize the present, and not appreciating talents.
        </div>
        <div class="content has-text-centered">
          <img src="images/CTC_Evaluation.png" width="75%">
          <p>Overall result of Chinese traditional painting.</p>
        </div>

      </div>
    </div>
<!-------------------------------------------------------------------- Error Analysis SECTION -------------------------------------------------------------------->
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Error Analysis</h2>
        <div class="content has-text-justified">
          <p>
            To conduct a comprehensive error analysis of GPT-4o's performance on CCPP-Bench, we randomly select a total of 100 erroneous samples from various domains, distributed according to their proportions in the dataset. 
            These samples are subjected to in-depth analysis by expert annotators. 
            GPT-4o's errors can be categorized into the following types: Information Neglect, Misunderstanding of Visual Information, Over-Inference, Superficial Reasoning, and Lack of Cultural Background Knowledge. 
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="images/error.png" alt="error distribution" width="50%">
          <p>GPT-4o error response distribution.</p>
        </div>
      </div>
    </div>

<!-------------------------------------------------------------------- Error Example  -------------------------------------------------------------------->

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="examples">Error Examples</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="error/case_study1.png" alt="grade-lv" width="73%">
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="error/case_study2.png" alt="grade-lv" width="73%">
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="error/case_study3.png" alt="grade-lv" width="73%">
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="error/case_study4.png" alt="grade-lv" width="73%">
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="error/case_study5.png" alt="grade-lv" width="73%">
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="error/case_study6.png" alt="grade-lv" width="73%">
            </div>
          </div>
          
        </div>
      </div>
    </div>

<!-------------------------------------------------------------------- Interpretability Analysis SECTION -------------------------------------------------------------------->
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3"> Interpretability Analysis of Chinese Image Implications</h2>
        <div class="content has-text-justified">
          <p>
            The essence of Chinese image implications is deeply rooted in deep cultural heritage and complex contextual associations, which enables them to convey profound messages through nuanced expressions. 
            For example, in traditional Chinese art forms such as landscape and New Year paintings, the imagery transcends mere depiction of nature or daily occurrences. 
            Instead, it embodies emotions, philosophical insights, and societal norms through metaphorical and highly symbolic expressions.
            These symbols, like the pine tree, plum blossom, and crane, are not superficial meaning but are steeped in centuries of cultural tradition, representing resilience, purity, and longevity.

            However, deciphering these complex messages can be challenging, particularly for those unfamiliar with the cultural and historical narratives tied to these symbols. 
            This contrasts with English image implications, which often convey messages through more straightforward and explicit symbolism.
            As a result, the interpretability of Chinese image implications depends to some extent on reconstructing and resonating with the cultural context, which is what makes them unique: their meaning is not only visual but also culturally resonant, bridging across time and space.

            Moreover, the interpretability of Chinese image implications has new changed in the modern era.
            Globalization and the surge of internet culture have intertwined foreign elements with traditional Chinese culture, birthing new symbols and implications. 
            This intersection introduces additional layers of meaning, complicating the interpretation of traditional symbols.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="images/Comparision of image.png" alt="Comparision of image" width="50%">
          <p>Comparision of Chinese and English image implications. 
            Chinese images often embody richer scenes and deeper implications with Chinese traditional culture compared with the straightforward and explicit symbolism in English images.</p>
        </div>
      </div>
    </div>

  </div>
</section>
<!-------------------------------------------------------------------- END SECTION -------------------------------------------------------------------->


